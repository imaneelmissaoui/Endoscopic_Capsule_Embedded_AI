{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8678d-8fd7-4b5c-a6e4-b8ef7da40a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow scikit-learn numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ce0e8-4381-49b9-8aff-9dad9041d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bb6a4-05a1-4df4-a08c-1398540fd442",
   "metadata": {},
   "source": [
    "### Loading Images, Boundaries and Masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4870d9f1-8559-4084-8672-957f0ef668f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 10:54:54.397904: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-19 10:54:54.434911: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-19 10:54:54.435851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-19 10:54:59.932179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_dir = './Data/WCEcurated/train'\n",
    "val_dir = './Data/WCEcurated/val'\n",
    "test_dir = './Data/WCEcurated/test'\n",
    "\n",
    "\n",
    "class_names = ['normal', 'ulcerative_colitis', 'polyps', 'esophagitis']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# load images and masks\n",
    "def load_images_and_labels(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, f'{label}_{class_name}')\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, image_name)\n",
    "            img = img_to_array(load_img(img_path, target_size=(224, 224)))\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load datasets\n",
    "train_images, train_labels = load_images_and_labels(train_dir)\n",
    "val_images, val_labels = load_images_and_labels(val_dir)\n",
    "test_images, test_labels = load_images_and_labels(test_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34168d-952b-49ff-a187-bfd2373c59dc",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b377bef-d469-4b05-a4ad-59e82884646e",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5057bbb-7b07-46d7-8ef4-a088bba9323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "val_images = val_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf14b1-dc63-4cec-b35f-cd521adacaf3",
   "metadata": {},
   "source": [
    "### Feature Extraction using MobileNetV2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f7c11-67dc-4547-bb43-2bfe3e865680",
   "metadata": {},
   "source": [
    "#### MobileNetV2 will be used to extract features from the lesion images (from the images folder). Since these images are in RGB, we will use the default configuration of MobileNetV2. \n",
    "Parameters:\n",
    "\n",
    "    weights='imagenet': Initializes the model with weights pre-trained on ImageNet.\n",
    "    include_top=False: Removes the final classification layers, enabling us to extract features instead.\n",
    "    input_shape=(224, 224, 3): Specifies the input shape for RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d367dfa-e7fd-42d2-86d8-735762e773ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 10:57:04.673493: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Load pre-trained MobileNetV2 without the classification head\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add a global average pooling layer to convert 4D feature maps to 2D feature vectors\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Create a feature extractor model\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd78ea94-cce6-4ee9-8774-368550082a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 25s 249ms/step\n",
      "63/63 [==============================] - 16s 248ms/step\n",
      "25/25 [==============================] - 6s 244ms/step\n"
     ]
    }
   ],
   "source": [
    "#Extracting Features\n",
    "train_features = feature_extractor.predict(train_images)\n",
    "val_features = feature_extractor.predict(val_images)\n",
    "test_features = feature_extractor.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71b924-7f01-428b-8706-c2a824de7baa",
   "metadata": {},
   "source": [
    "#### Train an SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57251fcf-49a5-4707-b91b-418cd68dda23",
   "metadata": {},
   "source": [
    "We will use an SVM classifier with an RBF kernel. Before feeding the features into the SVM, we standardize them using a StandardScaler.\n",
    "\n",
    "StandardScaler: Scales the features to have zero mean and unit variance, which improves the performance of SVM.\n",
    "\n",
    "SVC(kernel='rbf'): The RBF kernel is suitable for non-linear classification problems like this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c6c30b-54a7-4d0f-88e4-188202774895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svc&#x27;, SVC(gamma=&#x27;auto&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svc&#x27;, SVC(gamma=&#x27;auto&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(gamma=&#x27;auto&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a pipeline with standard scaling and an SVM classifier\n",
    "svm_model = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma='auto'))\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_model.fit(train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755cff7-506d-4211-b786-e24c0215cf20",
   "metadata": {},
   "source": [
    "#### Model evaluation \n",
    "Evaluate the trained SVM model on the validation and test datasets using accuracy and classification reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05d9e7b3-8c0f-4e5a-9969-1ef572f04e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.999375\n",
      "Training Precision: 0.9993757802746566\n",
      "Training Recall: 0.999375\n",
      "Training F1-score: 0.9993749997558592\n",
      "Training AUC: 0.9999977864583334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "###Training \n",
    "# Accuracy\n",
    "train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "\n",
    "# Precision, Recall, F1-score \n",
    "train_precision, train_recall, train_f1_score, _ = precision_recall_fscore_support(train_labels, train_predictions, average='macro')\n",
    "print(f\"Training Precision: {train_precision}\")\n",
    "print(f\"Training Recall: {train_recall}\")\n",
    "print(f\"Training F1-score: {train_f1_score}\")\n",
    "\n",
    "# AUC \n",
    "train_labels_bin = label_binarize(train_labels, classes=[0, 1, 2, 3])\n",
    "train_auc = roc_auc_score(train_labels_bin, svm_model.decision_function(train_features), average='macro')\n",
    "print(f\"Training AUC: {train_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03b31ce2-c9a5-4b1b-befa-a5050498ae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.95\n",
      "Validation Precision: 0.9540954179644814\n",
      "Validation Recall: 0.95\n",
      "Validation F1-score: 0.9495300731060741\n",
      "Validation  AUC: 0.993578\n"
     ]
    }
   ],
   "source": [
    "###Validation \n",
    "# Accuracy\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Precision, Recall, F1-score (Macro-average)\n",
    "val_precision, val_recall, val_f1_score, _ = precision_recall_fscore_support(val_labels, val_predictions, average='macro')\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-score: {val_f1_score}\")\n",
    "\n",
    "# AUC (Macro-average)\n",
    "val_labels_bin = label_binarize(val_labels, classes=[0, 1, 2, 3])\n",
    "val_auc = roc_auc_score(val_labels_bin, svm_model.decision_function(val_features), average='macro')\n",
    "print(f\"Validation  AUC: {val_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cee40cb-924a-4f14-9883-00bdc53120fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[200   0   0   0]\n",
      " [  0 162  37   1]\n",
      " [  0   3 197   0]\n",
      " [  0   0   0 200]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff5e910e-b3d1-4cdd-bd50-bcc4e3e36e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity for class 0: 1.0\n",
      "Sensitivity for class 1: 0.81\n",
      "Sensitivity for class 2: 0.985\n",
      "Sensitivity for class 3: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Compute recall (sensitivity) for each class\n",
    "sensitivity_per_class = recall_score(test_labels, test_predictions, average=None)\n",
    "\n",
    "# Print sensitivity for each class\n",
    "for i, sensitivity in enumerate(sensitivity_per_class):\n",
    "    print(f\"Sensitivity for class {i}: {sensitivity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d3fe8a-6e3f-4f60-9486-9a140f760af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity for class 0: 1.0\n",
      "Specificity for class 1: 0.995\n",
      "Specificity for class 2: 0.9383333333333334\n",
      "Specificity for class 3: 0.9983333333333333\n"
     ]
    }
   ],
   "source": [
    "specificity_per_class = []\n",
    "\n",
    "for i in range(len(conf_matrix)):\n",
    "    # True negatives: sum of all elements that are not in the i-th row or column\n",
    "    TN = conf_matrix.sum() - (conf_matrix[i, :].sum() + conf_matrix[:, i].sum() - conf_matrix[i, i])\n",
    "    # False positives: sum of the i-th column, excluding the diagonal element\n",
    "    FP = conf_matrix[:, i].sum() - conf_matrix[i, i]\n",
    "    # False negatives: sum of the i-th row, excluding the diagonal element\n",
    "    FN = conf_matrix[i, :].sum() - conf_matrix[i, i]\n",
    "    # True positives: diagonal element\n",
    "    TP = conf_matrix[i, i]\n",
    "    \n",
    "    # Specificity\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_per_class.append(specificity)\n",
    "\n",
    "# Print specificity for each class\n",
    "for i, specificity in enumerate(specificity_per_class):\n",
    "    print(f\"Specificity for class {i}: {specificity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e559cf-7107-450c-9a88-1e0e7c29f445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Error (Loss): 0.05125000000000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "# Classification error (loss)\n",
    "classification_error = 1 - accuracy\n",
    "print(f\"Classification Error (Loss): {classification_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c963fc7-9582-46e4-a83a-5eb6202617fc",
   "metadata": {},
   "source": [
    "#### Model optimization for Deployment \n",
    "To make the feature extraction model lightweight for embedded deployment, i converted it to TensorFlow Lite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf2169-52b2-4ec1-9f7d-c4a6a77de762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the feature extraction model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(feature_extractor)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open('./Model/mobilenetv2_feature_extractor.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7527f-1adc-44d8-a3f7-139bc46f2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained SVM model\n",
    "joblib.dump(svm_model, './Model/svm_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ff728-3904-4668-b53b-add4752369ee",
   "metadata": {},
   "source": [
    "## With Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6605ba95-104c-4806-91c8-04355199dfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copied to ./Data/WCEcurated_augmented\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "original_dataset_dir = './Data/WCEcurated'\n",
    "augmented_dataset_dir = './Data/WCEcurated_augmented'\n",
    "\n",
    "# Copy the dataset to a new directory to not change the initial dataset \n",
    "shutil.copytree(original_dataset_dir, augmented_dataset_dir)\n",
    "print(f\"Dataset copied to {augmented_dataset_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2f597a-df74-4baf-b93d-b5d21209dce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 16:53:19.667422: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-12 16:53:19.926707: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-12 16:53:19.928222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-12 16:53:21.969698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class '0_normal' has 1500 images, augmenting by 1500 images.\n",
      "Augmenting ./Data/WCEcurated_augmented/train/0_normal, current images: 800, need to add 800\n",
      "Augmenting ./Data/WCEcurated_augmented/val/0_normal, current images: 500, need to add 500\n",
      "Augmenting ./Data/WCEcurated_augmented/test/0_normal, current images: 200, need to add 200\n",
      "Class '1_ulcerative_colitis' has 1500 images, augmenting by 1500 images.\n",
      "Augmenting ./Data/WCEcurated_augmented/train/1_ulcerative_colitis, current images: 800, need to add 800\n",
      "Augmenting ./Data/WCEcurated_augmented/val/1_ulcerative_colitis, current images: 500, need to add 500\n",
      "Augmenting ./Data/WCEcurated_augmented/test/1_ulcerative_colitis, current images: 200, need to add 200\n",
      "Class '2_polyps' has 1500 images, augmenting by 1500 images.\n",
      "Augmenting ./Data/WCEcurated_augmented/train/2_polyps, current images: 800, need to add 800\n",
      "Augmenting ./Data/WCEcurated_augmented/val/2_polyps, current images: 500, need to add 500\n",
      "Augmenting ./Data/WCEcurated_augmented/test/2_polyps, current images: 200, need to add 200\n",
      "Class '3_esophagitis' has 1500 images, augmenting by 1500 images.\n",
      "Augmenting ./Data/WCEcurated_augmented/train/3_esophagitis, current images: 800, need to add 800\n",
      "Augmenting ./Data/WCEcurated_augmented/val/3_esophagitis, current images: 500, need to add 500\n",
      "Augmenting ./Data/WCEcurated_augmented/test/3_esophagitis, current images: 200, need to add 200\n",
      "Data augmentation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the dataset\n",
    "dataset_dir = './Data/WCEcurated_augmented'  # The dataset copy where i'll augment the data \n",
    "splits = ['train', 'val', 'test']  \n",
    "\n",
    "# Define ImageDataGenerator for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# augment images in a given directory\n",
    "def augment_images(class_dir, num_needed):\n",
    "    image_filenames = os.listdir(class_dir)\n",
    "    num_images = len(image_filenames)\n",
    "    \n",
    "    print(f\"Augmenting {class_dir}, current images: {num_images}, need to add {num_needed}\")\n",
    "    \n",
    "    for image_file in image_filenames:\n",
    "        img_path = os.path.join(class_dir, image_file)\n",
    "        img = load_img(img_path)\n",
    "        x = img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        \n",
    "        # Augment the image and save it to the class directory\n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1, save_to_dir=class_dir, save_prefix='aug', save_format='jpg'):\n",
    "            i += 1\n",
    "            if i >= num_needed:  \n",
    "                return\n",
    "\n",
    "# Main augmentation process\n",
    "for class_name in os.listdir(os.path.join(dataset_dir, 'train')):\n",
    "    total_images = 0\n",
    "    class_dirs = {split: os.path.join(dataset_dir, split, class_name) for split in splits}\n",
    "    \n",
    "    # Calculate total number of images for this class across all splits\n",
    "    for split, class_dir in class_dirs.items():\n",
    "        total_images += len(os.listdir(class_dir))\n",
    "    \n",
    "    # If total images are less than 3000, augment\n",
    "    if total_images < 3000:\n",
    "        num_to_add = 3000 - total_images\n",
    "        print(f\"Class '{class_name}' has {total_images} images, augmenting by {num_to_add} images.\")\n",
    "\n",
    "        # Distribute the augmentation across train, val, and test\n",
    "        for split, class_dir in class_dirs.items():\n",
    "            current_images = len(os.listdir(class_dir))\n",
    "            proportion = current_images / total_images if total_images > 0 else 1\n",
    "            num_needed = int(num_to_add * proportion)\n",
    "            \n",
    "            if num_needed > 0:\n",
    "                augment_images(class_dir, num_needed)\n",
    "\n",
    "print(\"Data augmentation complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8700f45e-bac2-4984-8769-670ceba01aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths to your dataset\n",
    "train_dir = './Data/WCEcurated_augmented/train'\n",
    "val_dir = './Data/WCEcurated_augmented/val'\n",
    "test_dir = './Data/WCEcurated_augmented/test'\n",
    "\n",
    "# Define class labels\n",
    "class_names = ['normal', 'ulcerative_colitis', 'polyps', 'esophagitis']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Function to load images and masks\n",
    "def load_images_and_labels(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, f'{label}_{class_name}')\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, image_name)\n",
    "            img = img_to_array(load_img(img_path, target_size=(224, 224)))\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load datasets\n",
    "train_images, train_labels = load_images_and_labels(train_dir)\n",
    "val_images, val_labels = load_images_and_labels(val_dir)\n",
    "test_images, test_labels = load_images_and_labels(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad7df864-28de-4b25-b896-1e73f14c0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "val_images = val_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99a5ab6e-0dcd-4296-9305-747a885b65d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 48s 243ms/step\n",
      "124/124 [==============================] - 30s 244ms/step\n",
      "50/50 [==============================] - 12s 244ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "# Load pre-trained MobileNetV2 without the classification head\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add a global average pooling layer to convert 4D feature maps to 2D feature vectors\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.8)(x)  # Increased dropout rate\n",
    "\n",
    "\n",
    "# Create a feature extractor model\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# `train_images`, `val_images`, and `test_images` are arrays of the augmented dataset images\n",
    "train_features = feature_extractor.predict(train_images)\n",
    "val_features = feature_extractor.predict(val_images)\n",
    "test_features = feature_extractor.predict(test_images)\n",
    "\n",
    "# Normalize the features if necessary\n",
    "train_features = np.array(train_features)\n",
    "val_features = np.array(val_features)\n",
    "test_features = np.array(test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b503b56-fdc6-4c9b-b0f2-8e388c79419a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svc&#x27;, SVC(gamma=&#x27;auto&#x27;, probability=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svc&#x27;, SVC(gamma=&#x27;auto&#x27;, probability=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(gamma=&#x27;auto&#x27;, probability=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto', probability=True))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a pipeline with standard scaling and an SVM classifier\n",
    "svm_model = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma='auto',probability=True))\n",
    "\n",
    "# Train the SVM classifier on the training features and labels\n",
    "svm_model.fit(train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef097102-1b33-43ca-8c57-6411cc43ef8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9998403830806065\n",
      "Validation Accuracy: 0.9777158774373259\n",
      "Training Precision: 0.9998404594767071\n",
      "Validation Precision: 0.977993790638009\n",
      "Training Recall: 0.9998402555910543\n",
      "Validation Recall: 0.9777220125316699\n",
      "Training AUC: 0.9999999320161811\n",
      "Validation AUC: 0.9978350986805634\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Make predictions and obtain probabilities\n",
    "train_predictions = svm_model.predict(train_features)\n",
    "val_predictions = svm_model.predict(val_features)\n",
    "test_predictions = svm_model.predict(test_features)\n",
    "\n",
    "# Obtain probability estimates\n",
    "train_proba = svm_model.predict_proba(train_features)\n",
    "val_proba = svm_model.predict_proba(val_features)\n",
    "\n",
    "# Calculate Accuracy\n",
    "train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "\n",
    "# Calculate Precision\n",
    "train_precision = precision_score(train_labels, train_predictions, average='macro')\n",
    "val_precision = precision_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "# Calculate Recall \n",
    "train_recall = recall_score(train_labels, train_predictions, average='macro')\n",
    "val_recall = recall_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "# Calculate AUC \n",
    "train_auc = roc_auc_score(train_labels, train_proba, multi_class='ovo', average='macro')\n",
    "val_auc = roc_auc_score(val_labels, val_proba, multi_class='ovo', average='macro')\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Training Precision: {train_precision}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Training Recall: {train_recall}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Training AUC: {train_auc}\")\n",
    "print(f\"Validation AUC: {val_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40fdb180-04a2-46d7-beac-af57bb58d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[393   6   0   0]\n",
      " [  0 369  27   1]\n",
      " [  0   4 394   0]\n",
      " [  0   0   0 397]]\n",
      "Specificity for class 0: 1.0\n",
      "Specificity for class 1: 0.9916247906197655\n",
      "Specificity for class 2: 0.9773679798826488\n",
      "Specificity for class 3: 0.9991624790619765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "#Specificity per class \n",
    "specificity_per_class = []\n",
    "\n",
    "for i in range(len(conf_matrix)):\n",
    "    # True negatives: sum of all elements that are not in the i-th row or column\n",
    "    TN = conf_matrix.sum() - (conf_matrix[i, :].sum() + conf_matrix[:, i].sum() - conf_matrix[i, i])\n",
    "    # False positives: sum of the i-th column, excluding the diagonal element\n",
    "    FP = conf_matrix[:, i].sum() - conf_matrix[i, i]\n",
    "    # False negatives: sum of the i-th row, excluding the diagonal element\n",
    "    FN = conf_matrix[i, :].sum() - conf_matrix[i, i]\n",
    "    # True positives: diagonal element\n",
    "    TP = conf_matrix[i, i]\n",
    "    \n",
    "    # Specificity\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_per_class.append(specificity)\n",
    "\n",
    "# Print specificity for each class\n",
    "for i, specificity in enumerate(specificity_per_class):\n",
    "    print(f\"Specificity for class {i}: {specificity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "314e0a0f-4511-41f6-9f93-9c4e399f10dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity for class 0: 0.9849624060150376\n",
      "Sensitivity for class 1: 0.929471032745592\n",
      "Sensitivity for class 2: 0.9899497487437185\n",
      "Sensitivity for class 3: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Compute recall (sensitivity) for each class\n",
    "sensitivity_per_class = recall_score(test_labels, test_predictions, average=None)\n",
    "\n",
    "# Print sensitivity for each class\n",
    "for i, sensitivity in enumerate(sensitivity_per_class):\n",
    "    print(f\"Sensitivity for class {i}: {sensitivity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imyenv",
   "language": "python",
   "name": "imyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
